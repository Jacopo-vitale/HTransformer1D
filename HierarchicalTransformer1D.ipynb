{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove these lines if not running on notebooks\n",
    "#%matplotlib notebook\n",
    "run_from_notebook = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required packages\n",
    "Insert here all the packages you require, so in case they are not found an error will be shown before any other operation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torchmetrics.functional import f1_score,matthews_corrcoef\n",
    "import utils.performance as pf\n",
    "from datetime import datetime as dt\n",
    "from h_transformer_1d import HTransformer1D\n",
    "from IPython.display import clear_output\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENCOVID custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GENCOVID(Dataset):\n",
    "    def __init__(self, experiment=\"DND\", gender=\"m\"):\n",
    "        # Load data\n",
    "        self.data    = pd.read_csv(\"./SienaNoiV2/data/FINAL_3/\" + experiment + \"_ds_\" + gender + \".csv\",header=0)\n",
    "        \n",
    "        # Set targets as patient grading\n",
    "        self.targets = pd.read_csv(\"./SienaNoiV2/data/FINAL_3/\" + experiment + \"_labels_\" + gender + \".csv\",header=0)\n",
    "        self.data.convert_dtypes(\"float\")\n",
    "        self.cols = self.data.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "    \n",
    "        return torch.LongTensor(self.data.iloc[idx]), torch.tensor(self.targets['grading'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"SNS\" #DND -> Dead or not Dead, \"SNS\" -> Severe or not Severe\n",
    "gender = \"f\"\n",
    "\n",
    "# Load Training Set\n",
    "dataset_train = GENCOVID(experiment=experiment,gender=gender)\n",
    "\n",
    "# Random split for retrieving Validation set\n",
    "torch.manual_seed(0)\n",
    "train_set_size = int(len(dataset_train) * 0.8)\n",
    "valid_set_size = len(dataset_train) - train_set_size\n",
    "\n",
    "#Using Validation as Test\n",
    "dataset_train,dataset_valid = random_split(dataset=dataset_train,lengths=[train_set_size,valid_set_size])\n",
    "\n",
    "# Some Debug\n",
    "print(\"length dataset_train:\",len(dataset_train))\n",
    "print(\"length dataset_valid:\",len(dataset_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters and options\n",
    "Set here your hyperparameters (to be used later in the code), so that you can run and compare different experiments operating on these values. \n",
    "<br>_Note: a better alternative would be to use command-line arguments to set hyperparameters and other options (see argparse Python package)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size        = 1\n",
    "learning_rate     = 1e-4\n",
    "epochs            = 100\n",
    "momentum          = 0.9\n",
    "lr_step_size      = 1000   # if < epochs, we are using decaying learning rate\n",
    "lr_gamma          = 0.01   #0.01\n",
    "data_augmentation = False   ##########################\n",
    "activation        = nn.GELU()\n",
    "input_dim         = dataset_train.dataset.cols\n",
    "num_tokens        = 256     # number of tokens\n",
    "dim               = 4      # dimension\n",
    "depth             = 1       # depths\n",
    "causal            = False   # autoregressive or not\n",
    "max_seq_len       = 128*128 # maximum sequence length\n",
    "heads             = 64       # heads\n",
    "dim_head          = 8      # dimension per head\n",
    "block_size        = 2       # block size\n",
    "reversible        = False   # use reversibility, to save on memory with increased depth\n",
    "shift_tokens      = False   # whether to shift half the feature space by one along the sequence dimension, for faster convergence (experimental feature)\n",
    "n_class           = 2\n",
    "\n",
    "# Create Hyperparameter Dictionary for Experiment Report\n",
    "hyperparameters = {'Batch Size'             : batch_size,\n",
    "                   'Learning Rate'          : learning_rate,\n",
    "                   'Epochs'                 : epochs,\n",
    "                   'Momentum'               : momentum,\n",
    "                   'Activation Fcn'         : activation,\n",
    "                   'Input Dimension'        : input_dim,\n",
    "                   'num_tokens'             : num_tokens,\n",
    "                   'dimension'              : dim,\n",
    "                   'depth'                  : depth,            \n",
    "                   'autoregressive or not'  : causal,           \n",
    "                   'maximum sequence length': max_seq_len,    \n",
    "                   'heads'                  : heads,\n",
    "                   'dimension per head'     : dim_head,\n",
    "                   'block size'             : block_size,\n",
    "                   'use reversibility'      : reversible,\n",
    "                   'whether to shift'       : shift_tokens}\n",
    "\n",
    "# make visible only one GPU at the time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # <-- should be the ID of the GPU you want to use\n",
    "# options\n",
    "device          = \"cuda:0\"  # put here \"cuda:0\" if you want to run on GPU\n",
    "monitor_display = True      # whether to display monitored performance plots\n",
    "display_first_n = 0         # how many samples/batches are displayed\n",
    "num_workers     = 2         # how many workers (=threads) for fetching data\n",
    "pretrained      = False     # whether to test a pretrained model (to be loaded) or train a new one\n",
    "display_errors  = True      # whether to display errors (only in pretrained mode)\n",
    "save_model_tar  = False     # Save the model for test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Transformer \n",
    "This is the architecture of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIGENTRA(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_tokens   : int,     # number of tokens\n",
    "                 dim          : int,     # dimension\n",
    "                 depth        : int,     # depth\n",
    "                 causal       : bool,    # autoregressive or not\n",
    "                 max_seq_len  : int,     # maximum sequence length\n",
    "                 heads        : int,     # heads\n",
    "                 dim_head     : int,     # dimension per head\n",
    "                 block_size   : int,     # block size\n",
    "                 reversible   : bool,    # use reversibility, to save on memory with increased depth\n",
    "                 shift_tokens : bool,    # whether to shift half the feature space by one along the sequence dimension, for faster convergence (experimental feature)\n",
    "                 n_class      : int,     # class number for classification\n",
    "                 activation   = nn.SiLU()\n",
    "    ):\n",
    "        super(HIGENTRA,self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        ## This is a Hierarchical 1D Transformer\n",
    "        Architecture:\n",
    "        -----------------------------------------\n",
    "        - Encoder --> Encode a tokenized 1D sequence into same size 1D sequence but containing Self-Attention Informations combined\n",
    "        - CLS Token --> Token added to tokenized sequence random initialized, used for classification purpose (see Vision Transformer)\n",
    "        - Multi Layer Perceptron --> Once selected the CLF token, MLP gives as output class confident scores\n",
    "        -----------------------------------------\n",
    "        Parameters:\n",
    "        num_tokens   : int,     # number of tokens\n",
    "        dim          : int,     # dimension\n",
    "        depth        : int,     # depth\n",
    "        causal       : bool,    # autoregressive or not\n",
    "        max_seq_len  : int,     # maximum sequence length\n",
    "        heads        : int,     # heads\n",
    "        dim_head     : int,     # dimension per head\n",
    "        block_size   : int,     # block size\n",
    "        reversible   : bool,    # use reversibility, to save on memory with increased depth\n",
    "        shift_tokens : bool,    # whether to shift half the feature space by one along the sequence dimension, for faster convergence (experimental feature)\n",
    "        n_class      : int,     # class number for classification\n",
    "        activation   = nn.SiLU()# Activation Function used for MLP Head\n",
    "        \"\"\"        \n",
    "        # Encoder\n",
    "        self.encoder = HTransformer1D(num_tokens = num_tokens, dim = dim,depth = depth, causal = causal,max_seq_len = max_seq_len, heads = heads, dim_head = dim_head, block_size = block_size, reversible = reversible, shift_tokens = shift_tokens)\n",
    "        \n",
    "        # MLP Head Classification\n",
    "        self.MLP = nn.Linear(dim,n_class)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = x[:,0,:]\n",
    "        \n",
    "        return self.MLP(x)\n",
    "\n",
    "        #return nn.functional.softmax(self.MLP(x),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the building blocks for training\n",
    "Create an instance of the network, the loss function, the optimizer, and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = HIGENTRA(num_tokens   = num_tokens,      # number of tokens\n",
    "               dim          = dim,             # dimension\n",
    "               depth        = depth,           # depth\n",
    "               causal       = causal,          # autoregressive or not\n",
    "               max_seq_len  = max_seq_len,     # maximum sequence length\n",
    "               heads        = heads,           # heads\n",
    "               dim_head     = dim_head,        # dimension per head\n",
    "               block_size   = block_size,      # block size\n",
    "               reversible   = reversible,      # use reversibility, to save on memory with increased depth\n",
    "               shift_tokens = shift_tokens,    # whether to shift half the feature space by one along the sequence dimension, for faster convergence (experimental feature)\n",
    "               n_class      = n_class,         # Number of classes\n",
    "               activation   = activation\n",
    "               )\n",
    "               \n",
    "# create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create SGD optimizer\n",
    "optimizer = optim.Adam(params=net.parameters(),lr=learning_rate) # optim.SGD(net.parameters(),lr=learning_rate,momentum=momentum) \n",
    "\n",
    "# create learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
    "\n",
    "# experiment ID\n",
    "experiment_ID = \"%s_%s_%s_bs(%d)lr(%.4f_%d_%.1f)m(%.1f)e(%d)act(%s)\" % (type(net).__name__ + gender.upper(), type(criterion).__name__, type(optimizer).__name__,\n",
    "                batch_size, learning_rate, lr_step_size, lr_gamma, momentum, epochs, type(activation).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.RandomApply(transforms=transforms.RandomHorizontalFlip(p=0.3),p=0.3)\n",
    "\n",
    "if data_augmentation:\n",
    "    dataset_train.transform = transform_train\n",
    "    print(f\"After Data Augmentation dataset length: {len(dataset_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loaders\n",
    "Dataloaders are in-built PyTorch objects that serve to sample batches from datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "# NOTE 1: shuffle helps training\n",
    "# NOTE 2: in test mode, batch size can be as high as the GPU can handle (faster, but requires more GPU RAM)\n",
    "# create dataset and dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train function\n",
    "It is preferable (but not mandatory) to embed training (1 epoch) code into a function, and call that function later during the training phase, at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train function (1 epoch)\n",
    "def train(dataset, dataloader):\n",
    "\n",
    "    # switch to train mode\n",
    "    net.train()\n",
    "\n",
    "    # reset performance measures\n",
    "    loss_sum = 0.0\n",
    "    \n",
    "    # initialize predictions\n",
    "    predictions    = torch.zeros(len(dataset), dtype=torch.int64)\n",
    "    groundtruth    = torch.zeros(len(dataset), dtype=torch.int64)\n",
    "    sample_counter = 0\n",
    "    target_counter = 0\n",
    "\n",
    "    # 1 epoch = 1 complete loop over the dataset\n",
    "    for batch in dataloader:\n",
    "\n",
    "        # get data from dataloader\n",
    "        inputs, targets = batch\n",
    "\n",
    "        # move data to device\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # loss gradient backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # net parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate loss\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        # store predictions\n",
    "        outputs_max = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        for output in outputs_max:\n",
    "            predictions[sample_counter] = output\n",
    "            sample_counter += 1\n",
    "        \n",
    "        for target in targets:\n",
    "            groundtruth[target_counter] = target\n",
    "            target_counter += 1    \n",
    "\n",
    "    # step learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # return average loss and accuracy\n",
    "    return loss_sum / len(dataloader), predictions, groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define test function\n",
    "It is preferable (but not mandatory) to embed the test code into a function, and call that function whenever needed. For instance, during training for validation at each epoch, or after training for testing, or for deploying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test function\n",
    "# returns predictions\n",
    "def test(dataset, dataloader):\n",
    "\n",
    "    # switch to test mode\n",
    "    net.eval()  \n",
    "\n",
    "    # initialize predictions\n",
    "    predictions = torch.zeros(len(dataset), dtype=torch.int64)\n",
    "    true_pred   = torch.zeros(len(dataset), dtype=torch.int64)\n",
    "    sample_counter = 0\n",
    "    target_counter = 0\n",
    "\n",
    "    # do not accumulate gradients (faster)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # test all batches\n",
    "        for batch in dataloader:\n",
    "\n",
    "            # get data from dataloader\n",
    "            inputs,targets = batch\n",
    "\n",
    "            # move data to device\n",
    "            inputs,targets = inputs.to(device, non_blocking=True),targets.to(device,non_blocking=True)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # store predictions\n",
    "            outputs_max = torch.argmax(outputs, dim=1)\n",
    "            for output in outputs_max:\n",
    "                predictions[sample_counter] = output\n",
    "                sample_counter += 1\n",
    "            \n",
    "            for target in targets:\n",
    "                true_pred[target_counter] = target\n",
    "                target_counter += 1            \n",
    "\n",
    "\n",
    "    return predictions, true_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a new model or test a pretrained one\n",
    "The code below also includes visual loss/accuracy monitoring during training, both on training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model not available --> TRAIN a new one and save it\n",
    "if not pretrained:\n",
    "\n",
    "    # Create performance folder\n",
    "    timestamp = (dt.now()).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = experiment + \"_\" + gender + \"_\" + timestamp\n",
    "    os.mkdir(path)\n",
    "\n",
    "    # reset performance monitors\n",
    "    losses = []\n",
    "    train_mccs = []\n",
    "    valid_mccs = []\n",
    "    ticks = []\n",
    "\n",
    "    with open(path + \"/\" + \"experiment_summary.txt\",mode='w', encoding=\"utf-8\") as f:\n",
    "        print(f\"***** HYPERPARAMETERS *****\",file=f)\n",
    "        \n",
    "        for key in hyperparameters:\n",
    "            print(f\"{key} : {hyperparameters[key]}\",file=f)\n",
    "        \n",
    "        print(f\"\\n\\n ***** MODEL ARCHITECTURE *****\\n {net}\",file=f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # move net to device\n",
    "    net.to(device)\n",
    "    # start training\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # measure time elapsed\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss,train_pred,train_true  = train(dataset_train, dataloader_train)\n",
    "\n",
    "        mcc_train = 100.* (train_pred.eq(train_true).sum().float())/len(dataset_train) \n",
    "\n",
    "        # F1-Score train\n",
    "        f1_train = 100.*f1_score(train_pred,train_true,average='micro')\n",
    "\n",
    "        # MCC Score Train\n",
    "        mcc_train = matthews_corrcoef(train_pred,train_true,2)\n",
    "\n",
    "\n",
    "        # test on validation\n",
    "        predictions,true_pred = test(dataset_valid, dataloader_valid)\n",
    "        mcc_valid = 100. * predictions.eq(true_pred).sum().float() / len(dataset_valid)\n",
    "          \n",
    "        # F1-Score validation \n",
    "        f1_valid = 100. * f1_score(predictions,true_pred,average='micro')\n",
    "        \n",
    "        # MCC Score Validation\n",
    "        mcc_valid = matthews_corrcoef(predictions,true_pred,2)\n",
    "\n",
    "        # update performance history\n",
    "        losses.append(avg_loss)\n",
    "        train_mccs.append(mcc_train.cpu())\n",
    "        valid_mccs.append(mcc_valid.cpu())\n",
    "        ticks.append(epoch)\n",
    "\n",
    "\n",
    "        # print or display performance\n",
    "        if not monitor_display:\n",
    "            print (\"\\nEpoch %d\\n\"\n",
    "                \"...TIME: %.1f seconds\\n\"\n",
    "                \"...loss: %g (best %g at epoch %d)\\n\"\n",
    "                \"...training accuracy: %.2f%% (best %.2f%% at epoch %d)\\n\"\n",
    "                \"...validation accuracy: %.2f%% (best %.2f%% at epoch %d)\" % (\n",
    "                epoch,\n",
    "                time.time()-t0,\n",
    "                avg_loss, min(losses), ticks[np.argmin(losses)],\n",
    "                mcc_train, max(train_mccs), ticks[np.argmax(train_mccs)],\n",
    "                mcc_valid, max(valid_mccs), ticks[np.argmax(valid_mccs)]))\n",
    "        else:\n",
    "            fig, ax1 = plt.subplots(figsize=(12, 8), num=1)\n",
    "            ax1.set_xticks(np.arange(0, epochs+1, step=epochs/10.0))\n",
    "            ax1.set_xlabel('Epochs')\n",
    "            ax1.set_ylabel(type(criterion).__name__, color='blue')\n",
    "            ax1.set_ylim(0.0001, 1)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(ticks, losses, 'b-', linewidth=1.0, aa=True, \n",
    "                label='Training (best at ep. %d)' % ticks[np.argmin(losses)])\n",
    "            ax1.legend(loc=\"lower left\")\n",
    "            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "            ax2.set_ylabel('MCC Score', color='red')\n",
    "            ax2.set_ylim(-1, 1)\n",
    "            ax2.set_yticks(np.arange(-1, 1, step=1e-1))\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.plot(ticks, train_mccs, 'r-', linewidth=1.0, aa=True, \n",
    "                label='Training (%.2f%%, best %.2f%% at ep. %d)' % (mcc_train, max(train_mccs), ticks[np.argmax(train_mccs)]))\n",
    "            ax2.plot(ticks, valid_mccs, 'r--', linewidth=1.0, aa=True, \n",
    "                label='Validation (%.2f%%, best %.2f%% at ep. %d)' % (mcc_valid, max(valid_mccs), ticks[np.argmax(valid_mccs)]))\n",
    "            ax2.legend(loc=\"lower right\")\n",
    "            plt.xlim(0, epochs+1)\n",
    "            # this works if running from notebooks\n",
    "            if run_from_notebook:\n",
    "                fig.show()\n",
    "                fig.canvas.draw()\n",
    "            # this works if running from console\n",
    "            else:\n",
    "                plt.draw()\n",
    "                plt.savefig(path + \"/\" + experiment_ID + \".png\", dpi=300)\n",
    "                plt.show()\n",
    "                clear_output(wait=True)\n",
    "            \n",
    "            fig.clear()\n",
    "\n",
    "        # save model if validation performance has improved\n",
    "        if (epoch-1) == np.argmax(valid_mccs):\n",
    "            # torch.save({\n",
    "            #     'net': net,\n",
    "            #     'accuracy': max(valid_accuracies),\n",
    "            #     'epoch': epoch\n",
    "            # }, experiment_ID + \".tar\")\n",
    "            # Performance Evaluation Training Set\n",
    "            pf.eval_performance(path            = path,\n",
    "                                experimentID    = experiment_ID,\n",
    "                                training        = True,\n",
    "                                hyperparameters = hyperparameters,\n",
    "                                y_pred          = train_pred, \n",
    "                                y_true          = train_true)\n",
    "                                    \n",
    "            # Performance Summary Validation Set\n",
    "            pf.eval_performance(path            = path,\n",
    "                                experimentID    = experiment_ID,\n",
    "                                training        = False,\n",
    "                                hyperparameters = hyperparameters,\n",
    "                                y_pred          = predictions.to(\"cpu\"), \n",
    "                                y_true          = true_pred.to(\"cpu\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
